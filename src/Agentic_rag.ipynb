{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain transformers sentence-transformers torch\n",
    "%pip install -qU langchain-chroma\n",
    "%pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d97bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f6d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Defining agent state\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    retrieved_docs: List[str]\n",
    "    final_answer: str\n",
    "    needs_retrieval: bool\n",
    "\n",
    "# Load Hugging Face LLM\n",
    "model_id = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "hf_pipeline = pipeline(\"text-generation\",\n",
    "                       model=model,\n",
    "                       tokenizer=tokenizer,\n",
    "                       max_new_tokens = 300,\n",
    "                       temperature = 0.2\n",
    "                       )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1c498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DGY3KOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Embedding Model\n",
    "embedder = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Provide the existing chroma vector store path\n",
    "VECTOR_DB_PATH = \"../Data/vectorstore\"\n",
    "vector_store = Chroma(\n",
    "    collection_name= \"pdf_documents\",  # Provide your collection name. Not sure of collection name, use the below code\n",
    "    # print(vector_store._collection.count())\n",
    "    persist_directory= VECTOR_DB_PATH,\n",
    "    embedding_function= embedder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc475d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH2021.1 Shapiro_Machine Learning ... 00e6 1 \n",
      "Machine Learning: what is it and what are its components? \n",
      "-- some preliminary observations1 \n",
      " \n",
      "Arnold F. Shapiro \n",
      "Penn State University, Smeal College of Business, University Park, PA 16802, USA \n",
      "Abstract \n",
      "This article focuses on conceptualizing machine learning (ML) concepts.  The general topics \n",
      "covered are supervised learning based on regression and classification, unsupervised \n",
      "learning based on clustering and dimensionality reduction, and rei\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\"What is machine learning\", k=1)\n",
    "print(results[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3999549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    This node is responsible ONLY for retrieval.\n",
    "    It does not generate answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the user query from agent state\n",
    "    query = state[\"user_query\"]\n",
    "\n",
    "    # Ask vector store for semantically similar chunks\n",
    "    results = vector_store.similarity_search(\n",
    "        query=query,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    # Extract raw text from Document objects\n",
    "    retrieved_texts = [doc.page_content for doc in results]\n",
    "\n",
    "    # Return partial state update (LangGraph merges state)\n",
    "    return {\n",
    "        \"retrieved_docs\": retrieved_texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16978692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create Decision node. Here it will check whether the RAG itself is enough or other tool needs to be called.\n",
    "def decide_node(state: AgentState):\n",
    "    prompt = f\"\"\"\n",
    "    User request:\n",
    "    {state['user_query']}\n",
    "    Do you need to retrieve external documents to answer this?\n",
    "    Answer only YES or NO.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt).strip().upper()\n",
    "\n",
    "    needs_retrieval = \"YES\" in response\n",
    "\n",
    "    return {\n",
    "        \"needs_retrieval\": needs_retrieval\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd55aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_node(state: AgentState):\n",
    "    context = \"\\n\\n\".join(state.get(\"retrieved_docs\", []))\n",
    "    print(\"=== RETRIEVED DOCS ===\")\n",
    "    for i, doc in enumerate(state.get(\"retrieved_docs\", [])):\n",
    "        print(f\"[{i}]\", doc[:300])\n",
    "    print(\"======================\")\n",
    "\n",
    "# Using Json format, we enforce structured JSON outputs from the LLM and parse them defensively to ensure deterministic agent behavior.\n",
    "    prompt = f\"\"\"\n",
    "        You are an AI assistant.\n",
    "\n",
    "        Your task:\n",
    "        - Answer the user's question using ONLY the context provided.\n",
    "        - Return the response in VALID JSON format.\n",
    "        - The JSON must contain exactly one key: \"answer\".\n",
    "        - Do NOT include any extra text outside JSON.\n",
    "        - Do NOT include explanations or metadata.\n",
    "\n",
    "        If the answer cannot be found in the context, return:\n",
    "        {{\"answer\": \"I do not have enough information to answer this.\"}}\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {state['user_query']}\n",
    "\n",
    "        JSON Response:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        answer = parsed.get(\"answer\", \"I do not have enough information to answer this.\")\n",
    "    except json.JSONDecodeError:\n",
    "        # Hard safety fallback\n",
    "        answer = \"I do not have enough information to answer this.\"\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": answer\n",
    "    }\n",
    "\n",
    "    # # HuggingFace models return full text; extract only the answer\n",
    "    # answer = response.split(\"Rules:\")[-1].strip()\n",
    "\n",
    "    # return {\n",
    "    #     \"final_answer\": answer\n",
    "    # }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de7ac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2b23e1956a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the LangGraph\n",
    "graph=StateGraph(AgentState)\n",
    "graph.add_node(\"decide\", decide_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"answer\", answer_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2cb4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_decision(state: AgentState):\n",
    "    if state[\"needs_retrieval\"]:\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        return \"answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccbb1d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2b23e1956a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will define control flow\n",
    "graph.set_entry_point(\"decide\")\n",
    "graph.add_conditional_edges(\n",
    "    \"decide\",\n",
    "    route_after_decision,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"answer\": \"answer\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"retrieve\", \"answer\")\n",
    "graph.add_edge(\"answer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a7d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RETRIEVED DOCS ===\n",
      "[0] s and theory that \n",
      "form the core of machine learning. Machine learning draws on concepts and \n",
      "results from many fields, including statistics, artificial intelligence, philosophy, \n",
      "information theory, biology, cognitive science, computational complexity, and \n",
      "control theory. My belief is that the bes\n",
      "[1] ARCH2021.1 Shapiro_Machine Learning ... 00e6 1 \n",
      "Machine Learning: what is it and what are its components? \n",
      "-- some preliminary observations1 \n",
      " \n",
      "Arnold F. Shapiro \n",
      "Penn State University, Smeal College of Business, University Park, PA 16802, USA \n",
      "Abstract \n",
      "This article focuses on conceptualizing machi\n",
      "[2] ning are summarized in Table 1.1. Langley \n",
      "and Simon (1995) and Rumelhart et al. (1994) survey additional applications of \n",
      "machine learning. \n",
      "This book presents the field of machine learning, describing a variety of \n",
      "learning paradigms, algorithms, theoretical results, and applications. Machine \n",
      "lea\n",
      "======================\n",
      "I do not have enough information to answer this.\n"
     ]
    }
   ],
   "source": [
    "# Compile and Run\n",
    "agent = graph.compile()\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"user_query\": \"What is machine Learning?\"\n",
    "    }\n",
    ")\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28017688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are receiving the right response because in chunking is very bad. Let's ee how to fix it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
