{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53ba859",
   "metadata": {},
   "source": [
    "### Sentence/Dynamic Chunking technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
      "Requirement already satisfied: nltk in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
      "Requirement already satisfied: fastapi in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.115.12)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi) (0.46.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dgy3kor\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bd7ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DGY3KOR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DGY3KOR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # It understands punctuation, abbreviations \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9e4ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DGY3KOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "PDF_FOLDER = \"../Data/PDFiles\"\n",
    "VECTOR_DB_PATH = \"../Data/Vectorstores\"\n",
    "COLLECTION_NAME = \"pdf_semantic_chunks_v1\"\n",
    "INGESTION_VERSION = \"v1\"\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str) -> list[tuple[int, str]]:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for page_number, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        if text and text.strip():\n",
    "            pages.append((page_number, text))\n",
    "\n",
    "    return pages\n",
    "        \n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5e67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build semantic chunking\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Dynamic chunking algorithm\n",
    "def split_into_paragraphs(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split cleaned text into paragraph-level chunks.\n",
    "    Paragraphs are better 'units of reasoning' for PDFs\n",
    "    than individual sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    paragraphs = [\n",
    "        p.strip()\n",
    "        for p in text.split(\"\\n\\n\")\n",
    "        if len(p.strip()) > 200  # filter headers / noise\n",
    "    ]\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "def is_definition_like(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detects whether a paragraph looks like a definition or explanation.\n",
    "    This metadata helps retrieval later.\n",
    "    \"\"\"\n",
    "\n",
    "    signals = [\n",
    "        \" is \",\n",
    "        \" refers to \",\n",
    "        \" defined as \",\n",
    "        \" means \",\n",
    "        \" consists of \",\n",
    "        \" can be described as \"\n",
    "    ]\n",
    "\n",
    "    lowered = text.lower()\n",
    "    return any(signal in lowered for signal in signals)\n",
    "\n",
    "def content_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a stable hash for deduplication.\n",
    "    Prevents repeated paragraphs from poisoning retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    normalized = \" \".join(text.lower().split())\n",
    "    return hashlib.sha256(normalized.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3567880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chunks to LangChain Documents\n",
    "def create_documents(chunks, source_file):\n",
    "    documents = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content=chunk, # What gets embedded\n",
    "                metadata={\n",
    "                    \"source\": source_file # Metadata is for debugging, citations, helpful for debugging\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5227d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion pipeline\n",
    "def ingest_pdfs():\n",
    "    all_documents = []\n",
    "    seen_hashes = set()\n",
    "\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "        print(f\"Ingesting: {filename}\")\n",
    "\n",
    "        pages = extract_pdf_pages(pdf_path)\n",
    "\n",
    "        for page_number, page_text in pages:\n",
    "            cleaned = clean_text(page_text)\n",
    "            paragraphs = split_into_paragraphs(cleaned)\n",
    "\n",
    "            for para in paragraphs:\n",
    "                h = content_hash(para)\n",
    "\n",
    "                # Cross-run + in-run deduplication\n",
    "                if h in seen_hashes:\n",
    "                    continue\n",
    "\n",
    "                seen_hashes.add(h)\n",
    "\n",
    "                all_documents.append(\n",
    "                    Document(\n",
    "                        page_content=para,\n",
    "                        metadata={\n",
    "                            \"source\": filename,\n",
    "                            \"page\": page_number,\n",
    "                            \"content_hash\": h,\n",
    "                            \"ingestion_version\": INGESTION_VERSION,\n",
    "                            \"type\": \"definition\" if is_definition_like(para) else \"general\"\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69c7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for Vectorstore\n",
    "def build_vectorstore(documents):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    vector_store = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=VECTOR_DB_PATH,\n",
    "        embedding_function=embedder\n",
    "    )\n",
    "\n",
    "    BATCH_SIZE = 500\n",
    "\n",
    "    for i in range(0, len(documents), BATCH_SIZE):\n",
    "        batch = documents[i : i + BATCH_SIZE]\n",
    "\n",
    "        ids = [\n",
    "            doc.metadata[\"content_hash\"]\n",
    "            for doc in batch\n",
    "        ]\n",
    "\n",
    "        vector_store.add_documents(\n",
    "            documents=batch,\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "        print(f\"Upserted batch {i // BATCH_SIZE + 1}\")\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d0795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting: arch-2021-1-machine-learning.pdf\n",
      "Ingesting: MachineLearningTomMitchell.pdf\n",
      "Total chunks created: 447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DGY3KOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted batch 1\n",
      "Vector store successfully built.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    documents = ingest_pdfs()\n",
    "\n",
    "    print(f\"Total chunks created: {len(documents)}\")\n",
    "    # print(f\"Page chunks: {len(page_chunks)}\")\n",
    "\n",
    "\n",
    "    vector_store = build_vectorstore(documents)\n",
    "\n",
    "    print(\"Vector store successfully built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b1396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "ARCH2021.1 Shapiro_Machine Learning ... 00e6 1 Machine Learning: what is it and what are its components? -- some preliminary observations1 Arnold F. Shapiro Penn State University, Smeal College of Business, University Park, PA 16802, USA Abstract This article focuses on conceptualizing machine learning (ML) concepts. The general topics covered are supervised learning based on regression and classification, unsupervised learning based on clustering and dimensionality reduction, and reinforcement learning. The article begins with an overview of ML, including the types of ML and their components. Then, for background, basic concepts associated with ML are addressed. From there on the discussion turns to ML algorithms, by topic. Although insurance applications of ML are not pursued in this version of the article, at the end of each section is a reference to an insurance article that applies the ML algorithm discussed in the section. The article ends with a commentary. 1 The support of the Risk Management Research Center at the Penn State University is gratefully acknowledged.\n",
      "----\n",
      "ARCH2021.1 Shapiro_Machine Learning ... 00e6 3 Figure 1: Traditional programming v. ML As indicated, in traditional programming, the data and the program are the inputs that the computer uses to produce the output. In contrast, under ML, the data and the output are the inputs to the computer, which uses them to develop a program to interrelate the two. 2.1 Types of ML ML is a subset of artificial intelligence (AI). In Figure 25 we address the topic of whether the AI topic under consideration is ML, and, if so, the type of ML. Figure 2: Is it Machine Learning (ML) and, if so, what type? As indicated, to qualify as ML, it must be seeking a pattern and there must be considerable data involved. If this is the situation, (1) It is supervised learning if it is told what to look for, that is, it is task driven. The computer is trained to apply a label to data. (2) It is reinforcement learning if trial and error are involved, in which case, the algorithm learns to react to the environment. The goal is to discover the best sequence of actions that will generate the optimal outcome. 5 Adapted from Hao (2019).\n",
      "----\n",
      "PREFACE The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience. In recent years many successful machine learning applications have been developed, ranging from data-mining programs that learn to detect fraudulent credit card transactions, to information-filtering systems that learn users' reading preferences, to autonomous vehicles that learn to drive on public highways. At the same time, there have been important advances in the theory and algorithms that form the foundations of this field. The goal of this textbook is to present the key algorithms and theory that form the core of machine learning. Machine learning draws on concepts and results from many fields, including statistics, artificial intelligence, philosophy, information theory, biology, cognitive science, computational complexity, and control theory. My belief is that the best way to learn about machine learning is to view it from all of these perspectives and to understand the problem settings, algorithms, and assumptions that underlie each. In the past, this has been difficult due to the absence of a broad-based single source introduction to the field. The primary goal of this book is to provide such an introduction. Because of the interdisciplinary nature of the material, this book makes few assumptions about the background of the reader. Instead, it introduces basic concepts from statistics, artificial intelligence, information theory, and other disci- plines as the need arises, focusing on just those concepts most relevant to machine learning. The book is intended for both undergraduate and graduate students in fields such as computer science, engineering, statistics, and the social sciences, and as a reference for software professionals and practitioners. Two principles that guided the writing of the book were that it should be accessible to undergrad- uate students and that it should contain the material I would want my own Ph.D. students to learn before beginning their doctoral research in machine learning.\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What is MAchine Learning?\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(\"----\")\n",
    "    print(r.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9afa0d",
   "metadata": {},
   "source": [
    "### Vector DB creation is done based on semantic meaning of sentences rather than explicit chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2e7bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.41.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: c:\\Users\\DGY3KOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83bbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "assert HF_TOKEN, \"Hugging Face API key not found\"\n",
    "\n",
    "# Defining agent state\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    retrieved_docs: List[str]\n",
    "    final_answer: str\n",
    "    needs_retrieval: bool\n",
    "    coverage: str\n",
    "\n",
    "# Load Hugging Face LLM\n",
    "# model_id = \"microsoft/phi-2\"\n",
    "hf_client = InferenceClient(\n",
    "    # model=\"deepseek-ai/DeepSeek-V3.2\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "\n",
    "def hf_chat(prompt: str) -> str:\n",
    "    response = hf_client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-V3.2\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Embedding Model\n",
    "embedder = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Provide the existing chroma vector store path\n",
    "VECTOR_DB_PATH = \"../Data/Vectorstores\"\n",
    "vector_store = Chroma(\n",
    "    collection_name= \"pdf_semantic_chunks_v1\",  # Provide your collection name. Not sure of collection name, use the below code\n",
    "    # print(vector_store._collection.count())\n",
    "    persist_directory= VECTOR_DB_PATH,\n",
    "    embedding_function= embedder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4113a8",
   "metadata": {},
   "source": [
    "### Build memory state for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ece0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    This node is responsible ONLY for retrieval.\n",
    "    It does not generate answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the user query from agent state\n",
    "    query = state[\"user_query\"]\n",
    "\n",
    "    # Ask vector store for semantically similar chunks\n",
    "    results = vector_store.similarity_search(\n",
    "        query=query,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    # Extract raw text from Document objects\n",
    "    retrieved_texts = [doc.page_content for doc in results]\n",
    "\n",
    "    # Return partial state update (LangGraph merges state)\n",
    "    return {\n",
    "        \"retrieved_docs\": retrieved_texts\n",
    "    }\n",
    "\n",
    "# Now we will create coverage node. Here it will check whether the RAG itself is enough or other tool needs to be called.\n",
    "def coverage_node(state: AgentState):\n",
    "    context = \"\\n\\n\".join(state.get(\"retrieved_docs\", []))\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are evaluating information coverage.\n",
    "        User question:\n",
    "        {state['user_query']}\n",
    "        Context:\n",
    "        {context}\n",
    "        Decide ONE label:\n",
    "        - DIRECT\n",
    "        - PARTIAL\n",
    "        - NONE\n",
    "        Return ONLY one word.\n",
    "        \"\"\"\n",
    "\n",
    "    response = hf_chat(prompt).upper()\n",
    "\n",
    "    if \"DIRECT\" in response:\n",
    "        coverage = \"DIRECT\"\n",
    "    elif \"PARTIAL\" in response:\n",
    "        coverage = \"PARTIAL\"\n",
    "    else:\n",
    "        coverage = \"NONE\"\n",
    "\n",
    "    return {\"coverage\": coverage}\n",
    "\n",
    "\n",
    "# If RAG provided chunks are not sufficient, then it should ask user.\n",
    "def ask_user_node(state: AgentState):\n",
    "    question = (\n",
    "        \"I found related information, but no single clear answer. \"\n",
    "        \"Would you like me to synthesize an explanation from multiple sections?\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": question\n",
    "    }\n",
    "\n",
    "def synthesize_node(state: AgentState):\n",
    "    context = \"\\n\\n\".join(state.get(\"retrieved_docs\", []))\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Synthesize an answer using ONLY the context.\n",
    "        Return JSON.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {state['user_query']}\n",
    "\n",
    "        {{\"answer\": \"...\"}}\n",
    "        \"\"\"\n",
    "\n",
    "    response = hf_chat(prompt)\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        answer = parsed.get(\"answer\")\n",
    "    except json.JSONDecodeError:\n",
    "        answer = \"I do not have enough information.\"\n",
    "\n",
    "    return {\"final_answer\": answer}\n",
    "\n",
    "\n",
    "def answer_node(state: AgentState):\n",
    "    context = \"\\n\\n\".join(state.get(\"retrieved_docs\", []))\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer the question using ONLY the context.\n",
    "Return VALID JSON.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{state['user_query']}\n",
    "\n",
    "JSON:\n",
    "{{\"answer\": \"...\"}}\n",
    "\"\"\"\n",
    "\n",
    "    response = hf_chat(prompt)\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        answer = parsed.get(\"answer\")\n",
    "    except json.JSONDecodeError:\n",
    "        answer = \"I do not have enough information to answer this.\"\n",
    "\n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34edf85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x16cb9f34b00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the LangGraph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"coverage\", coverage_node)\n",
    "graph.add_node(\"answer\", answer_node)\n",
    "graph.add_node(\"synthesize\", synthesize_node)\n",
    "\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"coverage\")\n",
    "\n",
    "def route_after_coverage(state: AgentState):\n",
    "    if state[\"coverage\"] == \"DIRECT\":\n",
    "        return \"answer\"\n",
    "    elif state[\"coverage\"] == \"PARTIAL\":\n",
    "        return \"synthesize\"\n",
    "    else:\n",
    "        return \"answer\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"coverage\",\n",
    "    route_after_coverage,\n",
    "    {\n",
    "        \"answer\": \"answer\",\n",
    "        \"synthesize\": \"synthesize\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"synthesize\", END)\n",
    "graph.add_edge(\"answer\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e16193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The types of machine learning are supervised learning and reinforcement learning. Supervised learning is when the system is told what to look for (task-driven) and is trained to apply a label to data. Reinforcement learning involves trial and error, where the algorithm learns to react to the environment to discover the best sequence of actions for an optimal outcome.\n"
     ]
    }
   ],
   "source": [
    "# Compile and Run\n",
    "agent = graph.compile()\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"user_query\": \"What are the types of machine learning?\"\n",
    "    }\n",
    ")\n",
    "print(result[\"final_answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
